import React, { useState, useEffect } from 'react';
import { ChartIcon, ArrowLeftIcon, CheckCircleIcon, RefreshIcon, ShieldExclamationIcon } from '../../../icons';
import { ChatPanel } from './ChatPanel';
import { ReportCanvas } from './ReportCanvas';
import { StepId, TechEvalSessionData, ChatMessage, ReportSection } from './types';
import { getPrompts, streamChatCompletions } from '../../../../api/stratify';
import { searchSemanticBatchGrouped } from '../../../../api/intelligence';
import { AGENTS } from '../../../../agentConfig';
import { StratifyPrompt } from '../../../../types';

interface TechDecisionAssistantProps {
    onBack?: () => void;
}

const DEFAULT_SECTIONS: Record<StepId, ReportSection> = {
    init: { id: 'init', title: 'åˆå§‹åŒ–', status: 'pending', markdown: '' },
    route: { id: 'route', title: 'æŠ€æœ¯è·¯çº¿', status: 'pending', markdown: '' },
    risk: { id: 'risk', title: 'é£é™©è¯„ä¼°', status: 'pending', markdown: '' },
    solution: { id: 'solution', title: 'è§£å†³æ–¹æ¡ˆ', status: 'pending', markdown: '' },
    compare: { id: 'compare', title: 'ç»¼åˆå†³ç­–', status: 'pending', markdown: '' },
};

const STEPS: StepId[] = ['init', 'route', 'risk', 'solution', 'compare'];
const DISPLAY_STEPS: StepId[] = ['route', 'risk', 'solution', 'compare'];
const SCENARIO_ID = 'd18630c7-d643-4a6d-ab8d-1af1731a35fb';

const RETRIEVAL_CONFIG = {
    threshold: 0.3,
    maxSegmentsPerQuery: 15 // æ¯ä¸ªç»´åº¦çš„ç‰‡æ®µæ•°ï¼Œæ‰¹é‡æ£€ç´¢æ€»æ•°çº¦ 40-100
};

const extractCleanHtml = (text: string) => {
    let cleanText = text.replace(/<think>[\s\S]*?<\/think>/gi, '');
    const codeBlockMatch = cleanText.match(/```html\s*([\s\S]*?)```/i);
    if (codeBlockMatch) return codeBlockMatch[1];
    
    const rawStart = cleanText.search(/<!DOCTYPE|<html|<div|<section/i);
    if (rawStart !== -1) return cleanText.substring(rawStart);
    return '';
};

const StepIndicator: React.FC<{ status: string, index: number, title: string, isActive: boolean }> = ({ status, index, title, isActive }) => {
    let colorClass = 'bg-slate-100 text-slate-400 border-slate-200';
    if (status === 'done') colorClass = 'bg-green-100 text-green-700 border-green-200';
    else if (status === 'generating' || status === 'review') colorClass = 'bg-indigo-100 text-indigo-700 border-indigo-200';
    
    return (
        <div className={`flex items-center gap-2 px-3 py-1.5 rounded-lg border text-xs font-bold transition-all whitespace-nowrap ${colorClass} ${isActive ? 'ring-2 ring-indigo-500/30 shadow-sm' : 'opacity-70'}`}>
            <span className="w-5 h-5 rounded-full bg-white/50 flex items-center justify-center text-[10px]">{index + 1}</span>
            <span className="hidden sm:inline">{title}</span>
            {status === 'generating' && <RefreshIcon className="w-3 h-3 animate-spin"/>}
            {status === 'done' && <CheckCircleIcon className="w-3.5 h-3.5"/>}
        </div>
    );
};

const TechDecisionAssistant: React.FC<TechDecisionAssistantProps> = ({ onBack }) => {
    const [data, setData] = useState<TechEvalSessionData>({
        techName: '',
        searchQueries: [],
        currentStepIndex: 0,
        sections: JSON.parse(JSON.stringify(DEFAULT_SECTIONS)),
        messages: [{
            id: 'welcome',
            role: 'assistant',
            content: 'æˆ‘æ˜¯æ‚¨çš„æŠ€æœ¯å†³ç­–è¯„ä¼°åŠ©æ‰‹ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³è¦è¯„ä¼°çš„æŠ€æœ¯åç§°ï¼ˆä¾‹å¦‚ï¼š800Vç¢³åŒ–ç¡…å¹³å°ã€åŠå›ºæ€ç”µæ± ç­‰ï¼‰ã€‚',
            timestamp: Date.now()
        }]
    });
    
    const [isGenerating, setIsGenerating] = useState(false);
    const [promptMap, setPromptMap] = useState<Record<string, StratifyPrompt>>({});
    const [isLoadingPrompts, setIsLoadingPrompts] = useState(true);
    const [promptError, setPromptError] = useState<string | null>(null);

    // å®šä¹‰æ´¾ç”ŸçŠ¶æ€ä»¥ä¿®å¤ Cannot find name 'currentStepId' å’Œ 'currentSection' é”™è¯¯
    const currentStepId = STEPS[data.currentStepIndex];
    const currentSection = data.sections[currentStepId];

    useEffect(() => {
        const loadPrompts = async () => {
            setIsLoadingPrompts(true);
            try {
                const fetchedPrompts = await getPrompts({ scenario_id: SCENARIO_ID });
                const map: Record<string, StratifyPrompt> = {};
                fetchedPrompts.forEach(p => {
                    map[p.name] = p;
                });
                setPromptMap(map);
            } catch (err: any) {
                setPromptError("åŠ è½½è¯„ä¼°æ¨¡å‹é…ç½®å¤±è´¥ã€‚");
            } finally {
                setIsLoadingPrompts(false);
            }
        };
        loadPrompts();
    }, []);

    const addMessage = (role: 'user' | 'assistant', content: string) => {
        const msg: ChatMessage = { id: crypto.randomUUID(), role, content, timestamp: Date.now() };
        setData(prev => ({ ...prev, messages: [...prev.messages, msg] }));
    };

    const updateSection = (stepId: StepId, updates: Partial<ReportSection>) => {
        setData(prev => ({
            ...prev,
            sections: {
                ...prev.sections,
                [stepId]: { ...prev.sections[stepId], ...updates }
            }
        }));
    };

    const getModelConfig = (promptName: string) => {
        const prompt = promptMap[promptName];
        if (!prompt) return null;
        let modelStr = 'zhipu@glm-4-flash'; 
        if (prompt.channel_code && prompt.model_id) {
            modelStr = `${prompt.channel_code}@${prompt.model_id}`;
        }
        return { contentTemplate: prompt.content, model: modelStr };
    };

    // --- Core Logic: Execute Batch Retrieval and Notify User ---
    const executeBatchRetrieval = async (queries: string[]): Promise<string> => {
        if (!queries || queries.length === 0) return "";

        // 1. Notify User: Start Batch Search
        const queryListStr = queries.map(q => `â€¢ ${q}`).join('\n');
        addMessage('assistant', `ğŸ” æ­£åœ¨æ‰§è¡Œå¤šç»´æ·±åº¦æ£€ç´¢ (Batch Mode)...\næ£€ç´¢ç»´åº¦æ¸…å•ï¼š\n${queryListStr}`);
        
        try {
            const response = await searchSemanticBatchGrouped({ 
                query_texts: queries, 
                similarity_threshold: RETRIEVAL_CONFIG.threshold,
                max_segments_per_query: RETRIEVAL_CONFIG.maxSegmentsPerQuery
            });

            const results = response.results || [];
            
            // 2. Format Context for LLM with clear classification
            let contextString = "";
            let totalSegmentsFound = 0;

            results.forEach(res => {
                const { query_text, items } = res;
                if (items && items.length > 0) {
                    contextString += `\n\nã€æ£€ç´¢ä¸»é¢˜ï¼š${query_text}ã€‘\n`;
                    items.forEach((article: any) => {
                        article.segments.forEach((seg: any, idx: number) => {
                            totalSegmentsFound++;
                            contextString += `èµ„æ–™[æ¥è‡ª:${article.source_name}]: ${seg.content}\n`;
                        });
                    });
                }
            });

            // 3. Notify User: Summary of findings
            if (totalSegmentsFound > 0) {
                 addMessage('assistant', `âœ… æ‰¹é‡æ£€ç´¢å®Œæˆï¼šå·²åœ¨çŸ¥è¯†åº“ä¸­æ•è· **${totalSegmentsFound}** ä¸ªé«˜ä»·å€¼æƒ…æŠ¥ç‰‡æ®µã€‚æ­£åœ¨åŸºäºåˆ†ç±»æ•°æ®è¿›è¡Œä¸“ä¸šåˆ†æ...`);
                 return contextString;
            } else {
                 addMessage('assistant', `âš ï¸ æ‰¹é‡æ£€ç´¢ç»“æŸï¼šå½“å‰çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°è¶³å¤ŸåŒ¹é…çš„ç»†èŠ‚ï¼ˆThreshold: ${RETRIEVAL_CONFIG.threshold}ï¼‰ã€‚å°†ç»“åˆè¡Œä¸šå¸¸è¯†ç”Ÿæˆè¯„ä¼°å»ºè®®ã€‚`);
                 return "";
            }
        } catch (e: any) {
            addMessage('assistant', `âŒ æ‰¹é‡æ£€ç´¢æœåŠ¡å¼‚å¸¸: ${e.message}ã€‚`);
            return "";
        }
    };

    const runInitStep = async (input: string) => {
        const config = getModelConfig('tech_eval_init');
        if (!config) {
            addMessage('assistant', `âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æç¤ºè¯é…ç½® [tech_eval_init]ã€‚`);
            return;
        }

        setIsGenerating(true);
        updateSection('init', { status: 'generating', usedModel: config.model });
        
        try {
            // Init phase still uses broad single retrieval to anchor tech name
            const ragContext = await executeBatchRetrieval([input]);
            const augmentedInput = ragContext ? `ç”¨æˆ·éœ€æ±‚: ${input}\n\nå‚è€ƒèƒŒæ™¯èµ„æ–™:\n${ragContext.slice(0, 3000)}` : input;
            const filledPrompt = config.contentTemplate.replace('{{ user_input }}', augmentedInput);

            let jsonBuffer = "";
            await streamChatCompletions({
                model: config.model,
                messages: [{ role: 'user', content: filledPrompt }],
                stream: true,
                temperature: 0.1
            }, (chunk) => {
                if (chunk.content) jsonBuffer += chunk.content;
            });

            let parsed;
            try {
                const match = jsonBuffer.match(/```json([\s\S]*?)```/) || jsonBuffer.match(/\{[\s\S]*\}/);
                const cleanJson = match ? match[0].replace(/```json/g, '').replace(/```/g, '') : jsonBuffer;
                parsed = JSON.parse(cleanJson);
            } catch (e) {
                parsed = { tech_name: input, search_queries: [input], definition: "è‡ªåŠ¨è§£æå¤±è´¥ã€‚" };
            }

            setData(prev => ({
                ...prev,
                techName: parsed.tech_name,
                techDefinition: parsed.definition,
                searchQueries: parsed.search_queries || [parsed.tech_name],
                currentStepIndex: 1,
            }));
            
            addMessage('assistant', `å·²ç¡®è®¤è¯„ä¼°å¯¹è±¡ï¼š**${parsed.tech_name}**ã€‚\n\næ­£åœ¨å¯åŠ¨ç¬¬ä¸€é˜¶æ®µåˆ†æï¼šæŠ€æœ¯è·¯çº¿ä¸ç«å“é”šå®š...`);
            setTimeout(() => runGenerationStep('route', parsed.tech_name, parsed.search_queries), 500);

        } catch (e: any) {
            addMessage('assistant', `åˆå§‹åŒ–å¤±è´¥: ${e.message}`);
        } finally {
            setIsGenerating(false);
            updateSection('init', { status: 'done' });
        }
    };

    const runGenerationStep = async (stepId: StepId, techName: string, queries: string[], userInstructions?: string) => {
        const promptKeyMap: Record<StepId, string> = {
            'init': 'tech_eval_init',
            'route': 'tech_eval_step1_route',
            'risk': 'tech_eval_step2_risk',
            'solution': 'tech_eval_step3_solution',
            'compare': 'tech_eval_step4_compare'
        };

        const config = getModelConfig(promptKeyMap[stepId]);
        if (!config) {
            addMessage('assistant', `âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æç¤ºè¯é…ç½® [${promptKeyMap[stepId]}]ã€‚`);
            return;
        }

        setIsGenerating(true);
        updateSection(stepId, { status: 'generating', markdown: '', usedModel: config.model });
        
        try {
            // Execute Batch Retrieval based on AI-generated search_queries
            let ragContext = "";
            if (!userInstructions) {
                // Use the structured queries generated during init phase
                ragContext = await executeBatchRetrieval(queries);
            } else {
                 ragContext = await executeBatchRetrieval([techName, userInstructions]);
            }

            const prevSummary = stepId === 'risk' ? data.sections['route'].markdown.slice(0, 500) :
                                stepId === 'solution' ? data.sections['risk'].markdown.slice(0, 500) :
                                stepId === 'compare' ? (data.sections['route'].markdown + data.sections['risk'].markdown + data.sections['solution'].markdown).slice(0, 1000) : '';

            let filledPrompt = config.contentTemplate
                .replace(/{{ tech_name }}/g, techName)
                .replace(/{{ retrieved_info }}/g, ragContext || 'æš‚æ— æ›´å¤šå¤–éƒ¨èµ„æ–™ã€‚')
                .replace(/{{ step1_summary }}/g, prevSummary)
                .replace(/{{ step2_summary }}/g, prevSummary)
                .replace(/{{ steps_summary }}/g, prevSummary);
            
            if (userInstructions) {
                filledPrompt += `\n\n**ç”¨æˆ·è¡¥å……æŒ‡ä»¤ï¼š**\n${userInstructions}`;
            }

            let fullContent = "";
            await streamChatCompletions({
                model: config.model,
                messages: [{ role: 'user', content: filledPrompt }],
                stream: true,
                temperature: 0.2
            }, (chunk) => {
                if (chunk.content) {
                    fullContent += chunk.content;
                    const cleanHtml = extractCleanHtml(fullContent);
                    updateSection(stepId, { markdown: fullContent, html: cleanHtml });
                }
            }, undefined, undefined, undefined, AGENTS.TECH_DECISION_ASSISTANT);

            updateSection(stepId, { status: 'review' });
            addMessage('assistant', `**${data.sections[stepId].title}** åˆ†æè‰ç¨¿å·²ç”Ÿæˆã€‚è¯·æŸ¥é˜…ã€‚`);

        } catch (e: any) {
            addMessage('assistant', `ç”Ÿæˆå¤±è´¥: ${e.message}`);
            updateSection(stepId, { status: 'pending' });
        } finally {
            setIsGenerating(false);
        }
    };

    const handleSendMessage = (text: string) => {
        addMessage('user', text);
        if (currentStepId === 'init') {
            runInitStep(text);
        } else if (currentSection.status === 'review') {
            runGenerationStep(currentStepId, data.techName, data.searchQueries, text);
        }
    };

    const handleConfirmStep = () => {
        updateSection(currentStepId, { status: 'done' });
        if (data.currentStepIndex < STEPS.length - 1) {
            const nextIndex = data.currentStepIndex + 1;
            setData(prev => ({ ...prev, currentStepIndex: nextIndex }));
            addMessage('assistant', `é˜¶æ®µç¡®è®¤ã€‚å¯åŠ¨ï¼š**${data.sections[STEPS[nextIndex]].title}**...`);
            setTimeout(() => runGenerationStep(STEPS[nextIndex], data.techName, data.searchQueries), 500);
        } else {
            addMessage('assistant', `ğŸ‰ æ­å–œï¼å…¨æµç¨‹è¯„ä¼°å·²å®Œæˆã€‚`);
        }
    };

    const handleRegenerateStep = () => {
        runGenerationStep(currentStepId, data.techName, data.searchQueries, "è¯·é‡æ–°ç”Ÿæˆæœ¬èŠ‚å†…å®¹ï¼Œå°è¯•æ›´æ·±å…¥çš„ç»´åº¦ã€‚");
    };

    if (isLoadingPrompts) return <div className="flex items-center justify-center h-full bg-[#f8fafc]"><RefreshIcon className="w-8 h-8 animate-spin text-indigo-600"/></div>;

    return (
        <div className="flex flex-col h-full bg-[#f8fafc]">
            <div className="h-16 px-6 border-b border-slate-200 bg-white/80 backdrop-blur-sm flex items-center justify-between shadow-sm z-10 flex-shrink-0">
                <div className="flex items-center gap-4">
                    <button onClick={onBack} className="p-2 -ml-2 text-slate-500 hover:bg-slate-100 rounded-full transition-colors group"><ArrowLeftIcon className="w-5 h-5 transition-transform group-hover:-translate-x-0.5" /></button>
                    <div className="h-6 w-px bg-slate-200"></div>
                    <div className="flex items-center gap-3">
                        <div className="w-8 h-8 rounded-lg bg-gradient-to-br from-indigo-500 to-purple-600 flex items-center justify-center text-white shadow-md"><ChartIcon className="w-4 h-4" /></div>
                        <h1 className="text-lg font-bold text-slate-800">æŠ€æœ¯å†³ç­–è¯„ä¼°åŠ©æ‰‹</h1>
                    </div>
                </div>
                <div className="flex items-center gap-6">
                    {data.techName && <div className="hidden md:flex items-center gap-2 bg-slate-100 px-3 py-1.5 rounded-lg border border-slate-200"><span className="text-xs text-slate-500 font-medium">è¯„ä¼°å¯¹è±¡:</span><span className="text-sm font-bold text-slate-800">{data.techName}</span></div>}
                    <div className="flex gap-2">
                        {DISPLAY_STEPS.map((step, idx) => <StepIndicator key={step} status={data.sections[step].status} index={idx} title={data.sections[step].title} isActive={currentStepId === step} />)}
                    </div>
                </div>
            </div>
            <div className="flex-1 flex overflow-hidden">
                <div className="flex-1 min-w-0 border-r border-slate-200 relative"><ReportCanvas sections={data.sections} currentStep={currentStepId} techName={data.techName} /></div>
                <div className="w-[450px] flex-shrink-0 bg-white shadow-xl z-10"><ChatPanel messages={data.messages} onSendMessage={handleSendMessage} isGenerating={isGenerating} currentStep={currentStepId} stepStatus={currentSection.status} onConfirmStep={handleConfirmStep} onRegenerateStep={handleRegenerateStep} /></div>
            </div>
        </div>
    );
};

export default TechDecisionAssistant;