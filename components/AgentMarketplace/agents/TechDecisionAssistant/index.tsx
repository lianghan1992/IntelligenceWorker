import React, { useState, useEffect } from 'react';
import { ChartIcon, ArrowLeftIcon, CheckCircleIcon, RefreshIcon, ShieldExclamationIcon } from '../../../icons';
import { ChatPanel } from './ChatPanel';
import { ReportCanvas } from './ReportCanvas';
import { StepId, TechEvalSessionData, ChatMessage, ReportSection } from './types';
import { getPrompts, streamChatCompletions } from '../../../../api/stratify';
import { searchSemanticBatchGrouped } from '../../../../api/intelligence';
import { AGENTS } from '../../../../agentConfig';
import { StratifyPrompt } from '../../../../types';

interface TechDecisionAssistantProps {
    onBack: () => void;
}

const DEFAULT_SECTIONS: Record<StepId, ReportSection> = {
    init: { id: 'init', title: 'åˆå§‹åŒ–', status: 'pending', markdown: '' },
    route: { id: 'route', title: 'æŠ€æœ¯è·¯çº¿', status: 'pending', markdown: '' },
    risk: { id: 'risk', title: 'é£é™©è¯„ä¼°', status: 'pending', markdown: '' },
    solution: { id: 'solution', title: 'è§£å†³æ–¹æ¡ˆ', status: 'pending', markdown: '' },
    compare: { id: 'compare', title: 'ç»¼åˆå†³ç­–', status: 'pending', markdown: '' },
};

const STEPS: StepId[] = ['init', 'route', 'risk', 'solution', 'compare'];
const DISPLAY_STEPS: StepId[] = ['route', 'risk', 'solution', 'compare'];
const SCENARIO_ID = 'd18630c7-d643-4a6d-ab8d-1af1731a35fb';

// æŒ‡å®šç”¨äºç”Ÿæˆæœç´¢å…³é”®è¯çš„æ¨¡å‹ï¼ˆä¸è®¡è´¹ï¼‰
const QUERY_REFINER_MODEL = "zhipu@glm-4-flash-250414";

const RETRIEVAL_CONFIG = {
    threshold: 0.3,
    maxSegmentsPerQuery: 12 // æ¯ä¸ªç»´åº¦çš„ç‰‡æ®µæ•°
};

const extractCleanHtml = (text: string) => {
    let cleanText = text.replace(/<think>[\s\S]*?<\/think>/gi, '');
    const codeBlockMatch = cleanText.match(/```html\s*([\s\S]*?)```/i);
    if (codeBlockMatch) return codeBlockMatch[1];
    
    const rawStart = cleanText.search(/<!DOCTYPE|<html|<div|<section/i);
    if (rawStart !== -1) return cleanText.substring(rawStart);
    return '';
};

const StepIndicator: React.FC<{ status: string, index: number, title: string, isActive: boolean }> = ({ status, index, title, isActive }) => {
    let colorClass = 'bg-slate-100 text-slate-400 border-slate-200';
    if (status === 'done') colorClass = 'bg-green-100 text-green-700 border-green-200';
    else if (status === 'generating' || status === 'review') colorClass = 'bg-indigo-100 text-indigo-700 border-indigo-200';
    
    return (
        <div className={`flex items-center gap-2 px-3 py-1.5 rounded-lg border text-xs font-bold transition-all whitespace-nowrap ${colorClass} ${isActive ? 'ring-2 ring-indigo-500/30 shadow-sm' : 'opacity-70'}`}>
            <span className="w-5 h-5 rounded-full bg-white/50 flex items-center justify-center text-[10px]">{index + 1}</span>
            <span className="hidden sm:inline">{title}</span>
            {status === 'generating' && <RefreshIcon className="w-3 h-3 animate-spin"/>}
            {status === 'done' && <CheckCircleIcon className="w-3.5 h-3.5"/>}
        </div>
    );
};

const TechDecisionAssistant: React.FC<TechDecisionAssistantProps> = ({ onBack }) => {
    const [data, setData] = useState<TechEvalSessionData>({
        techName: '',
        searchQueries: [],
        currentStepIndex: 0,
        sections: JSON.parse(JSON.stringify(DEFAULT_SECTIONS)),
        messages: [{
            id: 'welcome',
            role: 'assistant',
            content: 'æˆ‘æ˜¯æ‚¨çš„æŠ€æœ¯å†³ç­–è¯„ä¼°åŠ©æ‰‹ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³è¦è¯„ä¼°çš„æŠ€æœ¯åç§°ï¼ˆä¾‹å¦‚ï¼š800Vç¢³åŒ–ç¡…å¹³å°ã€åŠå›ºæ€ç”µæ± ç­‰ï¼‰ã€‚',
            timestamp: Date.now()
        }]
    });
    
    const [isGenerating, setIsGenerating] = useState(false);
    const [promptMap, setPromptMap] = useState<Record<string, StratifyPrompt>>({});
    const [isLoadingPrompts, setIsLoadingPrompts] = useState(true);

    const currentStepId = STEPS[data.currentStepIndex];
    const currentSection = data.sections[currentStepId];

    useEffect(() => {
        const loadPrompts = async () => {
            setIsLoadingPrompts(true);
            try {
                const fetchedPrompts = await getPrompts({ scenario_id: SCENARIO_ID });
                const map: Record<string, StratifyPrompt> = {};
                fetchedPrompts.forEach(p => {
                    map[p.name] = p;
                });
                setPromptMap(map);
            } catch (err) {
                console.error("Load prompts failed");
            } finally {
                setIsLoadingPrompts(false);
            }
        };
        loadPrompts();
    }, []);

    const addMessage = (role: 'user' | 'assistant', content: string) => {
        const msg: ChatMessage = { id: crypto.randomUUID(), role, content, timestamp: Date.now() };
        setData(prev => ({ ...prev, messages: [...prev.messages, msg] }));
    };

    const updateSection = (stepId: StepId, updates: Partial<ReportSection>) => {
        setData(prev => ({
            ...prev,
            sections: {
                ...prev.sections,
                [stepId]: { ...prev.sections[stepId], ...updates }
            }
        }));
    };

    const getModelConfig = (promptName: string) => {
        const prompt = promptMap[promptName];
        if (!prompt) return null;
        let modelStr = 'zhipu@glm-4-flash'; 
        if (prompt.channel_code && prompt.model_id) {
            modelStr = `${prompt.channel_code}@${prompt.model_id}`;
        }
        return { contentTemplate: prompt.content, model: modelStr };
    };

    /**
     * ä½¿ç”¨æŒ‡å®šæ¨¡å‹ (glm-4-flash-250414) æ‹†åˆ†å…³é”®è¯
     * ä¸è®¡è´¹
     */
    const refineSearchQueries = async (text: string): Promise<string[]> => {
        const prompt = `ä½ æ˜¯ä¸€ä¸ªæœç´¢ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹æŠ€æœ¯è¯„ä¼°éœ€æ±‚æ‹†åˆ†ä¸º 5-8 ä¸ªç‹¬ç«‹çš„è¯­ä¹‰æ£€ç´¢å…³é”®è¯ï¼Œç”¨äºå‘é‡æ•°æ®åº“æ£€ç´¢ã€‚
è¦æ±‚ï¼š
1. æ¶µç›–æŠ€æœ¯åŸç†ã€ç«å“åŠ¨æ€ã€å·¥ç¨‹é£é™©ã€ä¸“åˆ©ä¿¡æ¯ç­‰ç»´åº¦ã€‚
2. æ¯ä¸ªå…³é”®è¯åº”ç®€æ´ç²¾å‡†ã€‚
3. ä»…è¿”å› JSON å­—ç¬¦ä¸²æ•°ç»„ï¼Œå¦‚: ["å…³é”®è¯1", "å…³é”®è¯2"]

è¯„ä¼°éœ€æ±‚ï¼š${text}`;

        try {
            let buffer = "";
            await streamChatCompletions({
                model: QUERY_REFINER_MODEL,
                messages: [{ role: 'user', content: prompt }],
                stream: true,
                temperature: 0.1,
                enable_billing: false // æ˜ç¡®æŒ‡å®šä¸è®¡è´¹
            }, (chunk) => {
                if (chunk.content) buffer += chunk.content;
            });

            const match = buffer.match(/\[[\s\S]*\]/);
            if (match) {
                return JSON.parse(match[0]);
            }
        } catch (e) {
            console.warn("Refine queries failed, fallback to simple split");
        }
        return [text];
    };

    /**
     * æ‰§è¡Œæ‰¹é‡å‘é‡æ£€ç´¢å¹¶æ ¼å¼åŒ–è¾“å‡º
     */
    const executeBatchRetrieval = async (queries: string[]): Promise<string> => {
        if (!queries || queries.length === 0) return "";

        const queryListStr = queries.map(q => `â€¢ ${q}`).join('\n');
        addMessage('assistant', `ğŸ” æ­£åœ¨åŸºäºå¤šç»´è§†è§’æ‰§è¡Œæƒ…æŠ¥æ£€ç´¢...\n${queryListStr}`);
        
        try {
            const response = await searchSemanticBatchGrouped({ 
                query_texts: queries, 
                similarity_threshold: RETRIEVAL_CONFIG.threshold,
                max_segments_per_query: RETRIEVAL_CONFIG.maxSegmentsPerQuery
            });

            const results = response.results || [];
            let contextString = "";
            let totalSegmentsFound = 0;

            results.forEach(res => {
                const { query_text, items } = res;
                if (items && items.length > 0) {
                    contextString += `\n\nã€æ£€ç´¢å†…å®¹ï¼š${query_text}ã€‘\n`;
                    items.forEach((article: any) => {
                        article.segments.forEach((seg: any) => {
                            totalSegmentsFound++;
                            contextString += `- [æ¥è‡ª:${article.source_name}]: ${seg.content}\n`;
                        });
                    });
                }
            });

            if (totalSegmentsFound > 0) {
                 addMessage('assistant', `âœ… æ£€ç´¢å®Œæˆï¼šå·²æ•è· **${totalSegmentsFound}** æ¡ç»“æ„åŒ–æƒ…æŠ¥ã€‚æ­£åœ¨è¿›è¡Œæ·±åº¦æŠ€æœ¯è¯„ä¼°...`);
                 return contextString;
            } else {
                 addMessage('assistant', `âš ï¸ æ£€ç´¢ç»“æŸï¼šæœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°é«˜ç›¸å…³çš„æŠ€æœ¯ç»†èŠ‚ã€‚å°†åŸºäºè¡Œä¸šé€šç”¨çŸ¥è¯†è¿›è¡Œè¯„ä¼°ã€‚`);
                 return "";
            }
        } catch (e: any) {
            addMessage('assistant', `âŒ æ£€ç´¢æœåŠ¡å¼‚å¸¸: ${e.message}ã€‚`);
            return "";
        }
    };

    const runInitStep = async (input: string) => {
        const config = getModelConfig('tech_eval_init');
        if (!config) return;

        setIsGenerating(true);
        updateSection('init', { status: 'generating', usedModel: config.model });
        
        try {
            // æ­¥éª¤å‡†å¤‡ï¼šå…ˆæ‹†åˆ†å…³é”®è¯
            const refinedQueries = await refineSearchQueries(input);
            const ragContext = await executeBatchRetrieval(refinedQueries);
            
            const augmentedInput = ragContext ? `ç”¨æˆ·éœ€æ±‚: ${input}\n\nå‚è€ƒèƒŒæ™¯èµ„æ–™:\n${ragContext.slice(0, 4000)}` : input;
            const filledPrompt = config.contentTemplate.replace('{{ user_input }}', augmentedInput);

            let jsonBuffer = "";
            await streamChatCompletions({
                model: QUERY_REFINER_MODEL, // åˆå§‹åŒ–é˜¶æ®µä¹Ÿä½¿ç”¨è¯¥å…è´¹æ¨¡å‹ä»¥ç¡®ä¿ç¨³å®šæ€§
                messages: [{ role: 'user', content: filledPrompt }],
                stream: true,
                temperature: 0.1,
                enable_billing: false
            }, (chunk) => {
                if (chunk.content) jsonBuffer += chunk.content;
            });

            let parsed;
            try {
                const match = jsonBuffer.match(/\{[\s\S]*\}/);
                parsed = JSON.parse(match ? match[0] : jsonBuffer);
            } catch (e) {
                parsed = { tech_name: input, search_queries: refinedQueries, definition: "è§£æå¤±è´¥" };
            }

            setData(prev => ({
                ...prev,
                techName: parsed.tech_name,
                searchQueries: parsed.search_queries || refinedQueries,
                currentStepIndex: 1,
            }));
            
            addMessage('assistant', `è¯„ä¼°å¯¹è±¡ç¡®è®¤ï¼š**${parsed.tech_name}**ã€‚\n\nå¯åŠ¨ç¬¬ä¸€é˜¶æ®µï¼šæŠ€æœ¯è·¯çº¿æ·±åº¦è§£æ...`);
            setTimeout(() => runGenerationStep('route', parsed.tech_name, parsed.search_queries), 500);

        } catch (e: any) {
            addMessage('assistant', `åˆå§‹åŒ–å¤±è´¥: ${e.message}`);
        } finally {
            setIsGenerating(false);
            updateSection('init', { status: 'done' });
        }
    };

    const runGenerationStep = async (stepId: StepId, techName: string, queries: string[], userInstructions?: string) => {
        const promptKeyMap: Record<StepId, string> = {
            'init': 'tech_eval_init',
            'route': 'tech_eval_step1_route',
            'risk': 'tech_eval_step2_risk',
            'solution': 'tech_eval_step3_solution',
            'compare': 'tech_eval_step4_compare'
        };

        const config = getModelConfig(promptKeyMap[stepId]);
        if (!config) return;

        setIsGenerating(true);
        updateSection(stepId, { status: 'generating', markdown: '', usedModel: config.model });
        
        try {
            let activeQueries = queries;
            // å¦‚æœæ˜¯ç”¨æˆ·è¡¥å……æŒ‡ä»¤ï¼Œå…ˆåŠ¨æ€ç”Ÿæˆæ–°çš„æ£€ç´¢è¯
            if (userInstructions) {
                activeQueries = await refineSearchQueries(`${techName} ${userInstructions}`);
            }

            const ragContext = await executeBatchRetrieval(activeQueries);

            const prevSummary = stepId === 'risk' ? data.sections['route'].markdown.slice(0, 1000) :
                                stepId === 'solution' ? data.sections['risk'].markdown.slice(0, 1000) :
                                stepId === 'compare' ? (data.sections['route'].markdown + data.sections['risk'].markdown + data.sections['solution'].markdown).slice(0, 2000) : '';

            let filledPrompt = config.contentTemplate
                .replace(/{{ tech_name }}/g, techName)
                .replace(/{{ retrieved_info }}/g, ragContext || 'æš‚æ— å¤–éƒ¨è¡¥å……èµ„æ–™ã€‚')
                .replace(/{{ step1_summary }}/g, prevSummary)
                .replace(/{{ step2_summary }}/g, prevSummary)
                .replace(/{{ steps_summary }}/g, prevSummary);
            
            if (userInstructions) {
                filledPrompt += `\n\n**ç”¨æˆ·è¡¥å……è¦æ±‚ï¼š**\n${userInstructions}`;
            }

            let fullContent = "";
            await streamChatCompletions({
                model: config.model,
                messages: [{ role: 'user', content: filledPrompt }],
                stream: true,
                temperature: 0.2,
                enable_billing: true
            }, (chunk) => {
                if (chunk.content) {
                    fullContent += chunk.content;
                    const cleanHtml = extractCleanHtml(fullContent);
                    updateSection(stepId, { markdown: fullContent, html: cleanHtml });
                }
            }, undefined, undefined, undefined, AGENTS.TECH_DECISION_ASSISTANT);

            updateSection(stepId, { status: 'review' });
            addMessage('assistant', `**${data.sections[stepId].title}** åˆ†æè‰ç¨¿å·²å®Œæˆã€‚æ‚¨å¯ä»¥è¾“å…¥åé¦ˆè¿›è¡Œå¾®è°ƒï¼Œæˆ–ç›´æ¥ç¡®è®¤ã€‚`);

        } catch (e: any) {
            addMessage('assistant', `åˆ†æå¤±è´¥: ${e.message}`);
            updateSection(stepId, { status: 'pending' });
        } finally {
            setIsGenerating(false);
        }
    };

    const handleSendMessage = (text: string) => {
        addMessage('user', text);
        if (currentStepId === 'init') {
            runInitStep(text);
        } else if (currentSection.status === 'review') {
            runGenerationStep(currentStepId, data.techName, data.searchQueries, text);
        }
    };

    const handleConfirmStep = () => {
        updateSection(currentStepId, { status: 'done' });
        if (data.currentStepIndex < STEPS.length - 1) {
            const nextIndex = data.currentStepIndex + 1;
            setData(prev => ({ ...prev, currentStepIndex: nextIndex }));
            addMessage('assistant', `é˜¶æ®µå·²ç¡®è®¤ã€‚æ­£åœ¨å¯åŠ¨ï¼š**${data.sections[STEPS[nextIndex]].title}**...`);
            setTimeout(() => runGenerationStep(STEPS[nextIndex], data.techName, data.searchQueries), 500);
        } else {
            addMessage('assistant', `ğŸ‰ è¯„ä¼°æŠ¥å‘Šå…¨æµç¨‹å·²ç”Ÿæˆã€‚æ‚¨å¯ä»¥ç‚¹å‡»ä¸Šæ–¹æŒ‰é’®å¯¼å‡ºä¸º PDFã€‚`);
        }
    };

    const handleRegenerateStep = () => {
        runGenerationStep(currentStepId, data.techName, data.searchQueries, "è¯·é‡æ–°å®¡è§†ç°æœ‰æƒ…æŠ¥ï¼Œç»™å‡ºæ›´æ·±å…¥çš„ä¸“ä¸šåˆ†æã€‚");
    };

    if (isLoadingPrompts) return <div className="flex items-center justify-center h-full bg-[#f8fafc]"><RefreshIcon className="w-8 h-8 animate-spin text-indigo-600"/></div>;

    return (
        <div className="flex flex-col h-full bg-[#f8fafc]">
            <div className="h-16 px-6 border-b border-slate-200 bg-white/80 backdrop-blur-sm flex items-center justify-between shadow-sm z-10 flex-shrink-0">
                <div className="flex items-center gap-4">
                    <button onClick={onBack} className="p-2 -ml-2 text-slate-500 hover:bg-slate-100 rounded-full transition-colors group"><ArrowLeftIcon className="w-5 h-5 transition-transform group-hover:-translate-x-0.5" /></button>
                    <div className="h-6 w-px bg-slate-200"></div>
                    <div className="flex items-center gap-3">
                        <div className="w-8 h-8 rounded-lg bg-gradient-to-br from-indigo-500 to-purple-600 flex items-center justify-center text-white shadow-md"><ChartIcon className="w-4 h-4" /></div>
                        <h1 className="text-lg font-bold text-slate-800">æŠ€æœ¯å†³ç­–è¯„ä¼°åŠ©æ‰‹</h1>
                    </div>
                </div>
                <div className="flex items-center gap-6">
                    {data.techName && <div className="hidden md:flex items-center gap-2 bg-slate-100 px-3 py-1.5 rounded-lg border border-slate-200"><span className="text-xs text-slate-500 font-medium">è¯„ä¼°å¯¹è±¡:</span><span className="text-sm font-bold text-slate-800">{data.techName}</span></div>}
                    <div className="flex gap-2">
                        {DISPLAY_STEPS.map((step, idx) => <StepIndicator key={step} status={data.sections[step].status} index={idx} title={data.sections[step].title} isActive={currentStepId === step} />)}
                    </div>
                </div>
            </div>
            <div className="flex-1 flex overflow-hidden">
                <div className="flex-1 min-w-0 border-r border-slate-200 relative"><ReportCanvas sections={data.sections} currentStep={currentStepId} techName={data.techName} /></div>
                <div className="w-[450px] flex-shrink-0 bg-white shadow-xl z-10"><ChatPanel messages={data.messages} onSendMessage={handleSendMessage} isGenerating={isGenerating} currentStep={currentStepId} stepStatus={currentSection.status} onConfirmStep={handleConfirmStep} onRegenerateStep={handleRegenerateStep} /></div>
            </div>
        </div>
    );
};

export default TechDecisionAssistant;